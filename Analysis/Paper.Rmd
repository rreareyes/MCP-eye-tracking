---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "Ramiro Eduardo Rea Reyes"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "rreareyes@umass.edu"
    address       : ""
  - name          : "Youngbin Kwak"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Massachusetts, Amherst"


authornote: |
  X

abstract: |
  "X"
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : 
  - "References/Heuristics.bib"
  - "References/Eye-tracking_methods.bib" 
  - "References/software.bib"
  - "References/Modeling.bib"
  - "References/Instruments_surveys.bib"
  
appendix          :
  - "Scripts/M13_supplementary_figures.Rmd"

floatsintext      : no
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_docx
#   papaja::apa6_pdf:
# # output            : 
# #   papaja::apa6_docx:
#     includes:
#       after_body: "Scripts/D_supplementary_figures.Rmd"

header-includes:
     - \usepackage{longtable}
     - \usepackage{subcaption}
     - \usepackage{grffile}
   
---

```{r setup, include = FALSE}
library(papaja)
library(kableExtra)
library(tidyverse)

```

```{r base-paths, echo = F, message=F, warning=F}
folder_root <- getwd()

folder_references <- file.path(folder_root, "References")
folder_documents  <- file.path(folder_root, "Documents")
folder_scripts    <- file.path(folder_root, "Scripts")
folder_results    <- file.path(folder_root, "Results", "Summaries")
folder_figures    <- file.path(folder_root, "Figures")

r_refs(file = file.path(folder_references, "software.bib"))

software_citations <- cite_r(file     = file.path(folder_references, "software.bib"),
                             pkgs     = c("rstan", "brms"),
                             withhold = F)

# Load reporting functions
source(file = file.path(folder_scripts, "Functions", "F02_reporting.R"))

```

```{r stats-and-summaries, echo = F}
load(file.path(folder_results, "basic_summaries.RData"))
load(file.path(folder_results, "bayes_comparisons.RData"))
load(file.path(folder_results, "bayes_intervals.RData"))
load(file.path(folder_results, "bayes_rope.RData"))

```

# Introduction
Choosing between two options is arguably one of the most common decision scenarios we face in our daily life. Picking between two different sweets for dessert, wearing a white or blue shirt to a job interview, getting a cat or a dog, etc. All these seem like fairly easy decision to make, however, we can also imagine scenarios where the number of things to consider is not trivial. Buying a house located closer to work or closer to your friends; choosing between two universities who offered you acceptance letters; deciding to proceed with treatment A or B for a chronic disease, etc. 

These choices, beside varying on the specific topic they are made on, also differ in how much information an agent has before making her decision. @luceGamesDecisionsIntroduction1989 define 3 clear states of the world: certainty (the choice leads invariably to a known outcome), risk (the choice leads to a set of possible outcomes, each having a known probability of occurring), and uncertainty (each choice leads to one of a set of possible outcomes, whose probabilities are not known by the agent). 

How to study these decision scenarios is also a big discussion in itself. @savageFoundationsStatistics1954 made the distinction between small and large worlds in decision problems. Small worlds represent decision scenarios where the agent has complete knowledge of the possible outcomes of each action and its probabilities of occurrence (decisions under risk in the Luce-Raiffa framework). In large worlds on the other hand, some of the information (possible outcomes and/or its probabilities of occurring) is hidden to the agent, and she can only generate subjective estimates of the underlying probabilities at most (decisions under uncertainty). Although future research following @savageFoundationsStatistics1954, assumed that uncertain scenarios can be treated as risky by making some reasonable assumptions on the hidden probabilities from each outcome [@kozyrevaInterpretationUncertaintyEcological2019]. 

This whole idea lies at the center of the ecological rationality program. Here, what makes an agent and her decisions rational is not the degree of adherence with some normative benchmark, but their fit to the specific environment where the decision is taking place. This approach puts front and center the role of the environmental structure in determining what strategies are adequate to use [@gigerenzerSimpleHeuristicsThat1999]. It is important to distinguish here that this approach does not look for an optimal strategy. Under the consideration that the uncertainty on the decision environment cannot be reduced or simplified to risk, the optimal approach is unknown. Instead, under this framework, a *satisficing* strategy or strategies are considered as ecologically valid. Satisficing only implies to reach a given criterion of "good enough" performance, where the information accumulation process stops and the decision is made [@simonRationalChoiceStructure1956].

Now, considering both the constraints imposed from an uncertain environment, and the finite cognitive resources that can be invested, agents face a key problem: how much information should I gather before committing to one of the options? As we briefly touched before, in classic decision theory, we would determine an optimal performance level, based on an analytic utility function or in a probabilistic approximation of it[@radnerNormativeTheoryIndividual1986]. These optimal models represent a perfect observer that can integrate all the evidence in the environment and therefore determine which of the options maximizes the reward obtained at a given moment. Unsurprisingly, decision makers tend to underperform compared to these benchmarks, a fact that is widely observed in the decision-making literature [@gilovichHeuristicsBiasesPsychology2002]. Instead, agents tend to use simpler strategies that dismiss some of the information presented, reducing the search space. These strategies were referred as heuristics [@newellHumanProblemSolving1972; @tverskyJudgmentUncertaintyHeuristics1974; @simonInvariantsHumanBehavior1990], considered mere suboptimal shortcuts, inferior to their analytic and probabilistic counterparts.

The heuristics and biases framework, which spread from the work from @tverskyJudgmentUncertaintyHeuristics1974, focused on showing the systematic deviations of people's decision making process from the behavior that rational-agents should follow[@kahnemanMapsBoundedRationality2003]. The definition of rationality here is based on the degree of adherence of an agent's choices to normative models based in analytic or statistical solutions [@hammondHumanJudgmentSocial1996] in contrast the correspondence to the decision environment, as conceived within the ecological rationality framework. 

An alternative interpretation of these behaviors is offered by the idea of the adaptive toolbox developed by the ABC research group [@gigerenzerSimpleHeuristicsThat1999]. Here, a heuristic is defined as a strategy that aims to achieve decisions that are faster, frugal and/or more accurate than other complex methods, while ignoring certain parts of the information at hand [@gigerenzerHeuristicDecisionMaking2011]. This not only emphasizes the diminished effort for the decision maker, but also the possibility to be more accurate with less information, especially in scenarios with high levels of uncertainty, referred as the less-is-more effect. In line with ecological rationality, the decision agent would adaptively select the appropriate heuristic for the job from the available set of strategies in her toolbox.  

One of the most studied scenarios in the adaptive toolbox research uses binary choice paradigms [e.g., @broderDecisionMakingAdaptive2003; @dieckmannInfluenceInformationRedundancy2007; @rieskampSSLTheoryHow2006; @broderAssessingEmpiricalValidity2000; @leeEvidenceAccumulationDecision2004]. In these setups, the decision maker is presented with two options which have certain number of attributes to be evaluated. Each of those attributes tends to be a binary feature, i.e., having two possible states, but only one can be present at a time in a given option. Then, the participant needs to learn how to discern which alternative is superior, based on the sequential choices made and the observed outcome from these. Each of the different attribute's states has a validity associated with it, which is just the probability that the presence of a given feature predicts the correct option. 

There are multiple strategies that the agent can implement to solve these types of problems, each varying on the amount of information needed before making a choice. Broadly, we can divide these strategies in non-compensatory, relying in a single discerning feature to make their choice, and compensatory, more involved strategies that consider several features to determine the best option [@rieskampWhenPeopleUse1999]. 

A strong example of compensatory strategies is the linear integration of pieces of information, (Tallying) which is also referred in the literature as Weighted Additive (WADD) model, [@payneAdaptiveDecisionMaker1993], or the Franklin rule [@gigerenzerBettingOneGood1999]. This strategy integrates all the pieces of information available by first determining the winning states in each of the two options, then it multiplies the cues by their validities (subjective weights assigned by the agent) to finally add them up and select the option with the highest score. Despite it being a very successful way to approach most decision scenarios, its correct implementation is complicated and taxes highly the cognitive resources of the agent.

Another well studied compensatory strategy is the Dawes rule [@dawesLinearModelsDecision1974]. This heuristic also considers all the pieces of information, but instead of assigning a different subjective validity to each one of them, it weights them equally. In the end, it reflects a sum of the positive states of each cue, and then chooses the one with the highest score. This strategy is also fairly successful in both non-compensatory and compensatory environments and although it is less taxing, it is by no means frugal in the usage of information.

Among the non-compensatory strategies, the take-the-best (TTB) heuristic  [@gigerenzerReasoningFastFrugal1996] stands out both for its frugality in cue usage, as well as its lack of estimation of subjective weights to each piece of information. This decision model belongs to the "one reason decision making heuristics", since it relies in the discerning cue with the highest validity to make its choice. For this, it assumes that there is an underlying ranking among the cue validities. Its search starts from the most important feature and continues in descending order of importance, until a discriminating feature allows to distinguish between the options, choosing the one with the winning feature state. This strategy is very robust, and tends to surpass or match the accuracy from complex models like linear regression (i.e., Franklin rule) in multiple decision scenarios [@czerlinskiHowGoodAre1999]

There are several studies that have looked into what strategies and search patterns are implemented by decision makers to solve these paired comparison problems. However, there is a wide heterogeneity in terms of the environmental structure, information acquisition costs, and how the features were presented. All these can influence one way or another which strategy would be used more commonly among agents (as we would expect if we suppose their adaptive toolbox is sensitive to the underlying environmental structure).

For instance, adding costs to information acquisition either as monetary payments [e.g., @newellEmpiricalTestsFastandfrugal2003; @broderAssessingEmpiricalValidity2000; @rieskampSSLTheoryHow2006 experiment 3] or asking the participants to click with a mouse to reveal features [@payneAdaptiveStrategySelection1988; @rieskampSSLTheoryHow2006 experiments 1 and 2; @vanravenzwaaijHierarchicalBayesianModeling2014] tends to reduce the amount of information consulted by participants. Time pressure has shown a similar effect, biasing towards frugal strategies that ignore some pieces of information [@bobadilla-suarezFastFrugalNot2018; @oh-descherProbabilisticInferenceTime2017; @ohSatisficingSplitsecondDecision2016 TP stages]. Using paradigms that are based in presence/absence of binary features tend to bias agents to use strategies that count the present features [@leeEvidenceAccumulationDecision2004]. Moreover, presenting the cue rankings explicitly  or hinting the presence of a ranking [@ohSatisficingSplitsecondDecision2016 NP stages] seems to lean agents towards the usage of compensatory strategies, albeit this is not universally applied by all participants [@broderAssessingEmpiricalValidity2000; @nelsonTheoryHeuristicOptimal2018; @newellTakeBestLook2003].

In this work, we intend to provide an account of what are the naturally occurring strategies in naive agents and their prevalence to solve a two-option forced choice problem. Moreover, we want to present an environment similar to what an inexperienced decision maker may face. We do this by providing a full learning set where they must discover the structure of the environment while having limited search time that is tailored to their performance. Additionally, we aim to describe what search patterns these agents present by using an eye-tracker, an approach that we think provides more insight into the actual implementation of the decision strategies, while allowing them to sample without an explicit cost.

# Methods
## Participants
We collected a total of `r printnum(sum(summary_demographics$n))` participants from the student population of the University of Massachusetts, Amherst. These subjects participated in two separate experiments (Experiment 1 and Experiment 2; E1 and E2). From them `r printnum(sum(summary_demographics$n[summary_demographics$experiment == 1]))` participated in E1, `r printnum(filter(summary_demographics, experiment == "1") %>% filter(gender == "Female") %>% select(n) %>% .[[1]])` were females (mean age = `r filter(summary_demographics, experiment == "1") %>% filter(gender == "Female") %>% select(mean_age) %>% .[[1]]` SD = `r filter(summary_demographics, experiment == "1") %>% filter(gender == "Female") %>% select(sd_age) %>% .[[1]]`) and `r printnum(filter(summary_demographics, experiment == "1") %>% filter(gender == "Male") %>% select(n) %>% .[[1]])` were males (mean age = `r printnum(filter(summary_demographics, experiment == "1") %>% filter(gender == "Male") %>% select(mean_age) %>% .[[1]])`, SD = `r filter(summary_demographics, experiment == "1") %>% filter(gender == "Male") %>% select(sd_age) %>% .[[1]]`). For E2, we had a total of `r printnum(sum(summary_demographics$n[summary_demographics$experiment == 2]))` participants, from which `r printnum(filter(summary_demographics, experiment == "2") %>% filter(gender == "Female") %>% select(n) %>% .[[1]])` were females (mean age = `r filter(summary_demographics, experiment == "2") %>% filter(gender == "Female") %>% select(mean_age) %>% .[[1]]` SD = `r filter(summary_demographics, experiment == "2") %>% filter(gender == "Female") %>% select(sd_age) %>% .[[1]]`) and `r printnum(filter(summary_demographics, experiment == "2") %>% filter(gender == "Male") %>% select(n) %>% .[[1]])` were males (mean age = `r printnum(filter(summary_demographics, experiment == "2") %>% filter(gender == "Male") %>% select(mean_age) %>% .[[1]])`, SD = `r filter(summary_demographics, experiment == "2") %>% filter(gender == "Male") %>% select(sd_age) %>% .[[1]]`). We compensated all of them with a flat rate of class credit or \$12, as well as a monetary bonus of \$3 based on task performance. All participants provided informed consent in line with the Institutional Review Board from the University of Massachusetts, Amherst.

## Stimuli
We presented participants with cues composed by four different features inside a 2 by 2 grid (figure \@ref(fig:combinations)). These features corresponded to different items, each having binary states, in the form of two possible brands: helmet (JU/JO), fin (NE/NI), skate (BE/BA), and tire (WO/WI). We created the set of stimuli using all the combinations of these states, giving us a total of 16 unique grids ($4^2 = 16$). Additionally, we assigned complementary probabilistic weights to each feature's states. These weights predicted each state's probability of winning (figure \@ref(fig:weights)). The difference between the complementary weights is an objective measure of validity of each feature, i.e, how good a particular item is at predicting what option would be more likely to win the trial. The larger this difference is, the higher their relative importance compared to the other features. The position of the different items was the same for all participants, however we randomly assigned the complementary pairs of weights to different equipment for each participant, covering the 24 possible permutations of weights ($4!$). 

## Task
Our goal was to emulate some of the complexity present in real-world decision making. To achieve this, we presented participants with a probabilistic classification paradigm based on the work from @ohSatisficingSplitsecondDecision2016. In this task, the participants had to compare and choose between two options with multiple features. We created these pairs of cues by combining the whole set of 16 grids with each other. This yielded a total of 120 unique pairs of cues ($\frac{16^{2}}{2} - \frac{16}{2}$), and 240 by mirroring their position on the screen. We asked participants to compare the cues displayed, using the features in each of them to predict which of the options was more likely to win the trial. This bast array of cue and feature combinations, makes extremely difficult to make choices based only on memorizing the best combination. We can determine the probability that either side would win by comparing the weights of the feature states in the left cue $W_L = \{w_{C_{L,1}}, w_{C_{L,2}}, w_{C_{L,3}}, w_{C_{L,4}}\}$ with the ones in the right cue $W_R = \{w_{C_{R,1}}, w_{C_{R,2}}, w_{C_{R,3}}, w_{C_{R,4}}\}$ using the following equations: 

$$P(L|C_L, C_R) = \frac {10^{\sum_{n=1}^ {4} (w_{c_{L,i}} - w_{c_{R,i}})}} {1+10^{\sum_{n=1}^ {4} (w_{c_{L,i}} - w_{c_{R,i}})}}$$

$$P(R|C_L, C_R) = 1 - P(L|C_L,C_R)$$

Where *i* represents the ith feature inside each cue *c*. 

## Procedure
After signing the consent form, we provided the participants with instructions about the experiment. We framed our task as a race between two pilots who carried four different pieces of equipment, each of them with two possible brands. We explained that they would have to choose between two grids displayed on the screen, representing the different equipment of each pilot. We made clear they did not have any information about what combinations of items and brands were better, but that they would learn through trial and error as the task progressed. We did not provide any hint about the fact that the different features had a ranking among them, to avoid biasing participants towards implementing a search strategy relying on this. 

We built and presented our task using the Psychophysics Toolbox extension [@brainardPsychophysicsToolbox1997] in MATLAB. Additionally, we monitored participant's eye-movements using a SMI RED250mobile eye tracker (SensoMotoric Instruments) and the SMITE toolbox [@niehorsterSMITEToolboxCreating2020]. Each trial, we presented the participants with two cues on the right and left side of the screen (figure \@ref(fig:stimulus)). They had up to 15 seconds to choose one of these options, by pressing the corresponding arrow key on the keyboard. Upon making their choice or after the 15 seconds expired, both images disappeared from the screen. Immediately after this, we displayed a feedback message for 500 ms informing them about the outcome of their choice (win or lose) or a no response message (miss), as well as the number of points they earned. We assigned an ITI of 1 second between the feedback screen and the presentation of the next trial (figure \@ref(fig:sequence)A).

To include another level of uncertainty in our task, we made the outcome of each trial probabilistic: instead of informing participants if their choice was the cue with the highest probability of winning, we compared the probabilistic weight of the option they selected with a random draw from a uniform distribution between 0 and 1. If this weight was larger or smaller than this random number, then the trials was granted as a winning or losing trial, respectively. 

Furthermore, we wanted to encourage participants to perform the task as fast as they could to incentivize heuristics usage. However, using the same time pressure for all participants could be too restrictive for users of strategies sampling more information, and too lenient for more frugal strategies. Therefore, we imposed an adaptive time pressure based on the average RT from each participant. Winning trials with fast responses (1 SD below the mean RT) granted 2 points, whereas winning trials with slow responses (1 SD above the mean RT) granted 0 points (figure \@ref(fig:sequence)B). 

In both experiments, participants completed a total of 480 trials, organized in 8 blocks of 60 trials across 3 phases: a practice phase (P1) and two testing phases (T1 and T2). During P1 we presented participants with two full sets of 240 trials. By facing the whole array of stimuli, we allowed them to develop a decision strategy to implement in the following rounds. Both T1 and T2 presented 120 unique trials divided in two blocks (figure \@ref(fig:sequence)C). For T1, the contingencies assigned to the cue features remained the same as P1 (figure \@ref(fig:weights)). During T2 however, we introduced some changes to these: For Experiment 1, we set all the complementary weights from the brands in all the features to 0.80/0.20, preserving the winning states (figure \@ref(fig:weights-equal)). This change would benefit the performance of compensatory strategies that made use of more information from the cues, while hurting the success of frugal strategies that relied on fewer features. On the other hand, for Experiment 2 we inverted the ranking of the features, making the most informative piece of equipment (0.95/0.05) the least informative one (0.5001/0.4999) and so on (figure \@ref(fig:weights-inverted)). This change would hurt the most strategies relying on ranking the different pieces of information, whereas heuristics based on counting positive attributes without assigning an explicit ranking should increase their success or at least be resilient to the change. 

Finally, we determined a 400 points threshold for receiving the monetary bonus at the end of the task. This threshold is calculated based on winning 70% of the trials with 30% of them being fast responses.

## Data analysis

### Eye tracking
We identified fixations using the I2CM algorithm from @hesselsNoiserobustFixationDetection2017. To determine what features the participants were consulting to make their choices, we defined four AOIs within each cue, corresponding to the four pieces of equipment presented in the grids. We created these as squares with sides measuring 200 pixels, centered on each cell.

### Model classification
In order to identify what decision strategy was used by each participant, we implemented model classification using variational Bayesian inference [@drugowitschVariationalBayesianInference2019; @oh-descherProbabilisticInferenceTime2017; @ohSatisficingSplitsecondDecision2016]. This approach allows us to compare the support that different decision models have, given the choices made in each trial. For this we used only the trials from T2, since we considered that by this point the participants should have a more homogeneous strategy to make their choices. 

Given the binary nature of the decision (choosing left or right, represented as 1 and 0, respectively), we created different logistic models with different number of predictors, which represent the possible uses of the information available in the cues. These predictors can take the value of 1, when provide support for the left cue, 0 when both grids show the same brand, and -1 when the winning state is located in the option on the right.

Similar to @ohSatisficingSplitsecondDecision2016, we first defined an optimal model with four predictors, which correspond to the four pieces of equipment presented in each cue (figure \@ref(fig:decision-models), column 15).  This optimal model supposes full integration of the different features, allowing each of them to vary independently (i.e., each can have different importance in predicting a given participant's choices). We used this as the reference to compare all the other decision models, since it is the most flexible decision model to implement given the number of parameters to estimate. 

Then, we created 14 different models that represent the integration of different number of features: four single parameter models, which represent using a single piece of information to make a choice (figure \@ref(fig:decision-models), columns 1-4); six models that estimate 2 parameters (figure \@ref(fig:decision-models), columns 5-10) and  four model that represent the integration of 3 features before committing with one of the options (figure \@ref(fig:decision-models), columns 11-14).


However, as we mentioned before, this type of task can also elicit the usage of other decision strategies that involve using the information a bit different. First, we considered a Tallying strategy, which does not rely on establishing a ranking among the different features. As we mentioned before, this heuristic merely counts the number of positive attributes present in each cue (i.e., winning states/brands in the context of this task) and adds them up, choosing the option with the larger number. To represent this, we created a model with a single predictor that could take any integer from -4 to 4 (figure \@ref(fig:decision-models), column 16; all features are used, but they are encompassed in a single predictor, represented by the square shade surrounding the filled circles), representing the number of features favoring the left side (negative numbers would indicate support for the cue on the right).

Additionally, to represent the lexicographic strategy Take-the-best, we created a model with 4 different predictors that scans the features starting from the most predictive one, going down in order of importance, making a choice as soon as there is a difference in the feature states (figure \@ref(fig:decision-models), column 19). For example, a trial where the most important feature was favoring the left side would have [1, 0, 0, 0] as the vector of predictors, whereas a trial where the discerning feature was located in the third most important item would take the form [0, 0, -1, 0], favoring the cue on the right side. Therefore, this decision model would only technically use information of a single feature in each trial, but it would vary which item to consider depending on the particular set of cues compared. 

Finally, to account for models implemented in an incomplete fashion, we created the models represented in columns 17-18 on figure \@ref(fig:decision-models) to represent partial Tallying, and models 20-21 for incomplete TTB.

### Statistical comparisons
To estimate the differences in accuracy and search patterns among these decision groups, we performed Bayesian parameter estimation using `r software_citations`. Given the binary nature of the choice data (correct/incorrect), we implemented a hierarchical logistic model to estimate the performance across different phases. On the other hand, to process the eye-tracking data we used the fixations falling within the boundaries of any of the four AOIs within each cue, and labeling them accordingly. This gave us an outcome variable with four levels corresponding to each of the features composing the cues, for which we implemented hierarchical multinomial models to estimate the proportion of fixations directed to each feature under different conditions.

For all these models, we grouped participants in different decision groups, according to the results from the model classification. Given the individual differences that could be present in the data, even within groups, we included random effects and slopes for each participant, nested within the different decision strategies. To achieve reliable estimates for the standard deviation of the group parameters, we ran 4 chains with enough iterations to reach 10,000 ESS ($\approx 6000$ iterations per chain with 1000 for warmup). Given the complexity of the models, we assigned a small step size for the MCMC algorithm (0.99) to get rid of divergences while exploring the parameter space. Correspondingly, we also adjusted the treedepth (12-14) to allow the model to continue exploring for enough time. For covariance matrices between random intercepts and slopes, we used a mildly regularizing prior ($\rho = 2$). additionally, we used a mildly informative half Gaussian prior for the variability among participants and decision groups ($\mu = 0; \sigma = 2$).

Finally, to perform follow up comparisons, we used the posterior samples transformed to standardized difference (Cohen $\delta$). All these comparisons use a region of practical equivalence (ROPE) of 0.1 on a standardized scale for log odds, to denote a negligible effect (i.e., the area where we could conclude there is no difference between the elements compared).

# Results

## Strategy classification
Considering the fact that the implementation of different decision strategies has an impact on both the expected proportion of accurate choices across phases, and the fixation patterns used to sample information, we first divided participants according to the decision models that best fitted their responses. As we can see on figure \@ref(fig:classification), there is great heterogeneity in the strategies used by participants in this task. However, two of them seem to be predominant in both experiments: relying solely on the information from the most informative feature (1st only) and an incomplete integration of an equal weights to all the features (Partial Tallying). 

Besides these two strategies, we decided to examine the other decision schemes that would be more successful in this task. The first candidate for this was the lexicographic strategy Take-the-Best, for which there is widespread knowledge of its efficiency in non-compensatory binary decision paradigms like this [@gigerenzerHeuristicDecisionMaking2011]. Additionally, we included the incomplete version of this strategy, combining both into a "Serial Search" decision group. Next, we chose the full implementation of the Tallying decision model, to have as a point of comparison with its partial version. Finally, we decided to also include participants whose choices showed a strong reliance in the second most informative feature (2nd only), since this strategy should still yield reasonable performance on T1. On figure \@ref(fig:classification-reduced) we display this final set of decision strategies and their frequency across participants both experiments. 

## Performance
First, we wanted to assess participant's accuracy to identify and select the option with the highest chance of winning. For this, we took their responses and label them as correct when they selected the side with the highest probability of success, regardless if they won or lost that trial. We removed ambiguous cases where the probability for both sides was 0.50. Moreover, we wanted to provide estimates for the possible changes in this metric as a consequence of the different environments in E1 and E2. To do this, we ran a hierarchical logistic model, using both the experiment and phase as separate predictors to estimate the probability of choosing the winning side. Finally, since we thought that different decision strategies may be affected in distinct ways by these changes, we included in our model random effects and intercepts for each participant, nested within decision groups. For our priors, we used a Gaussian distribution for the effects and the intercept($\mu = 2; \sigma = 2$). We used these parameters, since we know that these strategies should perform well above chance level (0 in log odds), but we gave enough leeway for the model to explore different values, considering the potential drop after changing the environment in T2. 

Next, we extracted samples from the posterior for the accuracy of the different strategies in each phase for both E1 and E2. We can see that the changes in contingencies across T1 and T2 affected the decision groups differently (figure \@ref(fig:performance)). To review this in more detail, we looked at the difference in performance between the two phases (figure \@ref(fig:performance-change) and table \@ref(tab:table-performance-change)). These comparisons show that, during E1, only the Partial Tallying group showed a meaningful reduction on their performance (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "Partial Tallying")`). Additionally, participants on the Serial Search group also showed a negative trend in their performance (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "Serial Search")`) but there is too much uncertainty on its magnitude. 

On the other hand, E2 shows a stronger effect across decision groups, strongly decreasing the performance of 1st Only (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "1st only")`), Partial Tallying (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Partial Tallying")`), and Serial Search (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Serial Search")`). This confirms these strategies reliance on raking the different pieces of information to a certain degree, an approach that is severely punished by the environment on T2 in this experiment. On the other hand, participants classified as Tallying users show no meaningful change in their performance between phases (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Tallying")`). This suggest that they effectively integrated all pieces of information with equal weight, since inverting the ranking does not seem to affect their performance. Finally, despite the negative trend, we did not observe a meaningful decrease in accuracy for participants in the 2nd only strategy (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "2nd only")`). 

Moreover, when we compared the performance at T1 across strategy groups (figure \@ref(fig:performance-T1)) we found that Serial Search users tend to outperform more frugal strategies (1st only vs Serial Search: `r report(rope_performance_T1, comparison = "first_serial")`; 2nd only vs Serial Search: `r report(rope_performance_T1, comparison = "second_serial")`). Additionally, this strategy group has also a tendency to outperform more compensatory strategies (Serial Search vs Tallying `r report(rope_performance_T1, comparison = "serial_tally")`; Serial Search vs Partial Tallying `r report(rope_performance_T1, comparison = "serial_lazy")`) however there is a lot of uncertainty in our estimates as both of the 89% HDI still have some of their mass inside the designated ROPE. This is in line with the expected success that this type of lexicographic strategy tends to have in non-compensatory scenarios. All paired comparisons are reported in detail in table \@ref(tab:table-performance-T1)

## Fixation patterns 
Although the decision models we created considered a wide array of information usage, there are some underlying assumptions that can only be explored by actually looking at the sampling patterns from the participants. In general, these strategies have three main elements [@gigerenzerSimpleHeuristicsThat1999] that allow to describe them:

* A search rule entailing the direction of the sampling in the information space.
* A stopping rule that defines when the search stops.
* A decision rule that determines what option is selected based on the evidence collected.  

In the following analysis, we focus on the first two elements, which define the search patterns that the agent must follow under each strategy. For this we used a series of eye tracking benchmarks to evaluate the starting point, spread of search and stopping point of the different models during T1. 

To assess the starting point of the sampling process, we looked at the location of the first fixation in each trial. As described before, we considered fixations that landed inside any of the four feature AOIs we previously defined. Considering that we had 4 possible outcomes for our response variable, we estimated the proportion of first fixations allocated to each of the different features using a hierarchical multinomial model. Since we expected different behavior depending on the decision strategy implemented by the participants, we estimated variable intercepts for every one of them, nested within decision groups. Figure \@ref(fig:first) shows the proportion of first fixations allocated to each of the 4 features by order of importance. 

Next, to benchmark the spread of the sampling, we examined the overall allocation of fixations across the different cue features. For this we used a hierarchical multinomial model with 4 outcomes similar to the one described above but considering all the fixations from each trial in T1. Figure \@ref(fig:proportion) displays the overall allocation of fixations across features for each decision group. 

Finally, to review the stopping point of the sampling process, we looked at the location of the last fixation in each trial. Again, using a hierarchical multinomial model with 4 outcomes, but this time only considering the last fixation recorded in each trial. In figure \@ref(fig:last) we can see the distribution of last fixations across features.

Now, even though these metrics allow us to present a more detailed picture of the sampling process, they are insufficient to check its consistent application during the task. Moreover, the strategies using serial sampling change how long they search for information depending on the particular states present in each feature. Specifically, these assume that the participants would start searching in descending order of importance and stop sampling as soon as they find a difference between cues in the highest ranking feature. This means that we would expect more sampling for information as the best discriminating piece of equipment is located further down in the scale of importance. 

For these reasons, to analyze if this behavior was present in any of the decision strategies, we grouped trials according to which feature was discriminating between cues, in descending order of importance (\@ref(fig:scenarios)). This creates four decision scenarios:

* When the cue states in the 1st feature (ranked by importance) are different (A).
* When cue states are the same in 1st feature, but different in the 2nd (B).
* When cue states in both 1st and 2nd feature are the same, but the 3rd shows different states (C).
* All the feature but the least important have the same state (D). 

We added these as a predictor in our statistical models, and allowed its slope to vary across participants, nested within decision groups. We applied this to assess if this had an effect to both the start (figure \@ref(fig:first-scenario)), spread (figure \@ref(fig:proportion-scenario)), and end (figure \@ref(fig:last-scenario)) of the sampling process.

### First only
This decision strategy assumes that participants would only rely on the most informative feature to make their choices. This means that any instance where the most important cue is not different between cues, they should simply choose randomly between them. Therefore, we would expect a consistent preference towards the most informative feature in all the eye tracking measures. 

We found that participants in this group tend to direct their first fixation towards the most important feature more often (figure \@ref(fig:first-comparisons), panel A), in comparison to the other features (`r report(rope_first, strategy = "1st only")`). Using the decision scenario as a predictor in this metric (figure \@ref(fig:first-scenario-comparisons-A, panel A)) did not reveal any change in this behavior (`r report(rope_first_scenario, strategy = "1st only")`). This pattern repeats when we take a look to the overall allocation of fixations (figure \@ref(fig:proportion-comparisons), panel A), showing a higher proportion of them going towards the first feature compared to the rest of the information pieces (`r report(rope_proportion, strategy = "1st only")`). When we consider the different decision scenarios (figure \@ref(fig:proportion-scenario-comparisons-A) panel A), this result remains consistent, still favoring the most important feature (`r report(rope_first_scenario, strategy = "1st only")`). Moreover, when we look at the stopping point for this strategy (figure \@ref(fig:last-comparisons), panel A), we also see a preference for the most important feature (`r report(rope_last, strategy = "1st only")`). Finally, this effect is still present when we introduce the decision scenario as a predictor (figure \@ref(fig:last-scenario-comparisons-A), panel A), still displaying their preference towards the most informative feature (`r report(rope_last_scenario, strategy = "1st only")`). Overall, the different eye-tracking benchmarks from this set of participants reveal a complete focus on the most important piece of information across the task, disregarding most of the other features in the cues.

### Second only
This strategy is similar to the previously described, differing only in its reliance instead on the second most informative feature. This can happen as an overestimation of its importance by the participants using this strategy, leading them to think this feature is the most predictive of success in the task. Again, this should be reflected by a consistent preference for the second most important feature, which should not be affected by decision scenarios.

As expected, we see how these subjects preferred the 2nd most important feature as their starting point for their sampling (figure \@ref(fig:first-comparisons), panel B; `r report(rope_first, strategy = "2nd only")`). This effect remains when looking at the different decision scenarios (figure \@ref(fig:proportion-scenario-comparisons-A), where we find a preference for this feature over the others in all but one scenario (`r report(rope_first_scenario, strategy = "2nd only")`). Looking at the sampling spread for this strategy, it shows us an overall preference for the second most important feature (figure \@ref(fig:proportion-comparisons), panel B; `r report(rope_proportion, strategy = "2nd only")`). This pattern holds for scenario A and B, but the scenario C and D, while they show a probability of direction favoring the 2nd feature, they still show high uncertainty in its magnitude (figure \@ref(fig:proportion-scenario-comparisons-A); `r report(rope_proportion_scenario, strategy = "2nd only")`). Finally, the stopping point for this strategy shows a tendency to favor the expected feature, however, this difference overlaps with the defined ROPE, which indicates this may not be a meaningful effect (figure \@ref(fig:last-comparisons), panel B; `r report(rope_last, strategy = "2nd only")`). When we look more closely at each decision scenario, we can see that the last fixation only shows a meaningful effect favoring the 2nd feature in scenario B (figure \@ref(fig:last-scenario-comparisons-A), panel B; `r report(rope_last_scenario, strategy = "2nd only")`). These results do show a reliance on the second most informative feature, however, the fixation patterns reveal more heterogeneity in the way the participants collected information from the cues, compared to the strong behavior shown by users of the 1st only strategy.

### Serial Search
So far, the strategies analyzed predict static fixation patterns, always preferring a particular feature over the others. For users of strategies that rely in conditional sampling, the preferred piece of information changes depending on the feature states presented in a given trial. In particular, for the participants grouped in our Serial Search group, we predict they should start by looking at the most important feature, comparing it between cues and only continue sampling if this does not discriminate between the two options. Therefore, we expect an overall preference for the most important feature in all the metrics we used, however, as we divide the trials by decision scenarios, we would expect that participants using this strategy should allocate more fixations to the other features, and they should finish their sampling further down in the feature rankings.

Our results show that in effect, these participants start on the most important feature (figure \@ref(fig:first-comparisons), panel C; `r report(rope_first, strategy = "Serial Search")`), remaining constant across decision scenarios (figure \@ref(fig:first-scenario-comparisons-A), panel C; `r report(rope_first_scenario, strategy = "Serial Search")`). For the proportion of fixations, we were interested in testing whether participants allocated their fixations following a linear trend, for which purpose we performed a series of paired comparisons (figure \@ref(fig:proportion-comparisons)). We found meaningful differences between the most important levels (1st vs 2nd: `r report(rope_proportion, strategy = "Serial Search", comparison = "d1_2")`; 2nd vs 3rd: `r report(rope_proportion, strategy = "Serial Search", comparison = "d2_3")`), but not for the 3rd vs 4th (`r report(rope_proportion, strategy = "Serial Search", comparison = "d3_4")`). When comparing the magnitude of these differences, we did not find any differences between them, indicating a linear relationship between fixation proportions and feature importance from the 1st to the 3rd most important feature (`r report(rope_proportion, strategy = "Serial Search", comparison = "lin1")`). Looking at the sampling across scenarios (figure \@ref(fig:proportion-scenario-comparisons-B), second section), we can see that the fixations in scenario A, favor the most important feature (`r report(rope_proportion_scenario, strategy = "Serial Search", scenario = "0")`). In scenario B, the difference between the fixations to feature 1 and 2 narrows, making it not meaningful anymore (`r report(rope_proportion_scenario, strategy = "Serial Search", scenario = "1", comparison = "d1_2")`), while still being larger than the proportion of fixations directed to the least informative features (`r report(rope_proportion_scenario, strategy = "Serial Search", scenario = "1", comparison = "d12_34")`). This pattern stops when we reach scenario C, where the proportion of fixations towards feature 1 still is larger than to feature 3 (`r report(rope_proportion_scenario, strategy = "Serial Search", scenario = "2", comparison = "d1_3")`), which repeats again in scenario D (`r report(rope_proportion_scenario, strategy = "Serial Search", scenario = "3", comparison = "d1_3")`). This suggest that the amount of sampling only equalizes for the two most important features and does not occur as consistently for the scenarios where the best discriminating feature is located further down in the scale, more consistent with an incomplete use of the features. Finally, the analysis of last fixations (figure \@ref(fig:last-comparisons), panel C) does not reveal an overall linear trend following the importance of the different domains (see table \@ref(tab:table-last)). However, a more meaningful description for the stopping point of this group is to look across decision scenarios (figure \@ref(fig:last-scenario-comparisons), third section). This benchmark shows a preference for the most important feature in scenario A (`r report(rope_last_scenario, strategy = "Serial Search", scenario = "0")`), while in scenario B we can see participants stopping more frequently on 2nd most important feature (`r report(rope_last_scenario, strategy = "Serial Search", scenario = "1")`). Scenario C displays a preference for the 3rd most important feature (`r report(rope_last_scenario, strategy = "Serial Search", scenario = "2")`) however, scenario D does not show most of the fixations landing on the least important feature (`r report(rope_last_scenario, strategy = "Serial Search", scenario = "3")`).

### Tallying
This strategy despite using all the cue features before choosing one of the options, integrates them in a single criterion. Instead of considering each piece of information with its own weight, i.e., assigning them a ranking according to how good they predict the winning choice, it weights them equally and measures the support by counting the number of features favoring each side. Considering this, we should see no difference between the allocation of fixations to the different features, reflecting an equal consideration of all of them by the participants. Moreover, the sampling should remain pretty much the same across decision scenarios, since these conditions should bear no effect in how many features they consult to make their choices.

We can see that the starting point (figure \@ref(fig:first-comparisons), panel D) does not seem to be directed to any of the features in particular. However, there is a trend favoring the most informative feature compared to the 2nd (`r report(rope_first, strategy = "Tallying", comparison = "d1_2")`), 3rd (`r report(rope_first, strategy = "Tallying", comparison = "d1_3")`), and 4th (`r report(rope_first, strategy = "Tallying", comparison = "d1_4")`), but the uncertainty associated to its estimate does not allow us to mark it as meaningful. Looking at this metric across decision scenarios does not show any disruption in the general pattern observed previously (figure \@ref(fig:first-scenario-comparisons-B), second section). Additionally, the overall allocation of fixations does not seem to lean towards any of the features in particular (figure \@ref(fig:proportion-comparisons), panel D). However, when we analyze these comparisons across decision scenarios (figure \@ref(fig:proportion-scenario-comparisons), third section), we can see a meaningful difference between the allocation of fixations to feature 1 and 4 in scenario A, favoring the latter (`r report(rope_proportion_scenario, strategy = "Tallying", scenario = "0", comparison = "d1_4")`). This does indicate at least a distinction between the most informative and least informative features from the participants classified in this group by the choice model. This observation is reinforced by the results on their stopping point (figure \@ref(fig:proportion-scenario-comparisons), panel D), showing a meaningful difference which favors feature 1 when compared to feature 4 (`r report(rope_last, strategy = "Tallying", comparison = "d1_4")`). Furthermore, the analysis of the location of last fixation in the decision scenarios (figure \@ref(fig:last-scenario-comparisons), third section) shows a meaningful difference that favors the most informative feature over the 2nd (`r report(rope_last_scenario, strategy = "Tallying", scenario = "0", comparison = "d1_2")`), 3rd (`r report(rope_last_scenario, strategy = "Tallying", scenario = "0", comparison = "d1_3")`), and 4th (`r report(rope_last_scenario, strategy = "Tallying", scenario = "0", comparison = "d1_4")`) features in scenario A. This suggests some underlying knowledge of the different predictive importance of the pieces of equipment inside the cues, even if their choices in the end reflect an equal consideration of all these features.

### Partial Tallying
Finally, this decision strategy only differs from its more compensatory parent in the number of features added up to determine the cue with more positive attributes. In this particular case, the participants classified in this group only integrate the 1st and 2nd most important features, therefore identifying explicitly these two as superior sources of information compared to the rest cue components. Still, these two features share the same level in the hierarchy, making both equally important to determine their choice. Therefore, we can expect different results on the fixation benchmarks we review here. Overall, we should see a preference for the two most important features in all the metrics when compared to the least informative ones, however, we should see no difference between the 1st and 2nd feature, given its equal subjective importance.

For the starting point, we can see that this strategy does not show the expected pattern of favoring the most important features over the least important ones (figure \@ref(fig:first-comparisons), panel E; `r report(rope_first, strategy = "Partial Tallying", comparison = "d12_34")`). The different scenarios do not make these comparisons change in a meaningful way (figure \@ref(fig:first-scenario-comparisons-C), third section). The overall proportion of fixations for this strategy (figure \@ref(fig:proportion-comparisons), panel E) does show the predicted preference towards the most important features (F1-F2 vs F3-4, `r report(rope_proportion, strategy = "Partial Tallying", comparison = "d12_34")`), while there is no meaningful difference between feature 1 and 2 (`r report(rope_proportion, strategy = "Partial Tallying", comparison = "d1_2")`), and features 3 and 4 (`r report(rope_proportion, strategy = "Partial Tallying", comparison = "d3_4")`). This pattern only holds for scenario A (`r report(rope_proportion_scenario, strategy = "Partial Tallying", comparison = "d12_34", scenario = "0")`) and B (`r report(rope_proportion_scenario, strategy = "Partial Tallying", comparison = "d12_34", scenario = "1")`). Finally, the stopping point (figure \@ref(fig:last-comparisons), panel E) also displays a preference towards the most important features (`r report(rope_last, strategy = "Partial Tallying", comparison = "d12_34")`), while also not showing any difference between feature 1 and 2 (`r report(rope_last, strategy = "Partial Tallying", comparison = "d1_2")`) or feature 3 and 4 (`r report(rope_last, strategy = "Partial Tallying", comparison = "d3_4")`). When looking at the differences across scenarios (figure \@ref(fig:last-scenario-comparisons), fourth section), this pattern only holds in scenario A (F1-F2 vs F3-F4, `r report(rope_last_scenario, strategy = "Partial Tallying", comparison = "d12_34", scenario = "0")`; F1 vs F2, `r report(rope_last_scenario, strategy = "Partial Tallying", comparison = "d1_2", scenario = "0")`; F3 vs F4, `r report(rope_last_scenario, strategy = "Partial Tallying", comparison = "d3_4", scenario = "0")`). 

# Discussion

**Just some notes:**

[@juslinPROBabilitiesEXemplarsPROBEX2002] there is a lot of cognitive resources invested in determining the ranking of the information.

Review from [@broderChallengingCommonBeliefs2008]

**Saved from the intro**

Simon (1955) integrated the concept of decisions under uncertainty into what he called bounded rationality. Moreover, in further works, he extended the concept of uncertainty in decision making to encompass not only the constraints caused by incomplete information, but also the computational limitations of the agent to fully process it, making it very hard to reliably determine the underlying structure of the environment (Simon, 1972). These limitations present a clear problem for the underlying assumptions made in classical definitions of rationality, which rely in the use of logic and probability under small worlds. Therefore, a new interpretation of rationality needed to consider the inseparable nature of the two blades that shape rationality according to Simon's analogy: "the structure of the task environment and the computational capabilities of the actor" (Simon, 1990).

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# Figures and Tables

<!-- Basic information -->

```{r combinations, fig.cap = "Arrangement features and states inside within cues.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_combinations.png"))

```

```{r stimulus, fig.cap = "Sample stimulus presented to participants.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_stimulus.png"))

```

```{r weights, fig.cap = "Example of the organization of cue weights across features during training and T1. All the equipment (features) and brands (states) kept their position constant across subjects, but the allocation of weights was assigned semi randomly across them.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_weights_normal.png"))

```

```{r sequence, fig.cap = "General overview of the task sequence. Panel A shows the presentation timing of the different stages of each trial. In Panel B we can see the different possibilities of feedback received by the participants. Lastly, panel C summarizes the trial composition of each phase.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_sequence.png"))

```


```{r weights-equal, fig.cap = "Example of the organization of cue weights across features during T2 in 0. All the different pairs had the same contingencies, preserving winning states.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_weights_equal.png"))

```

```{r weights-inverted, fig.cap = "Example of the organization of cue weights across features during T2 in 1. The different features show now inverted contingencies, preserving winning states.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_weights_inverted.png"))

```

```{r decision-models, fig.cap = "Information usage in the different decision models. In models 1-15, black circles indicate the feature(s) used by each strategy. For models 16-18 we created a model that integrated the features in a single piece of information (i.e., a sum of positive traits). Models 19-21 consider a Serial Search for information, starting in the most informative feature, continuing in descending order of importance, and stopping as soon as any of these features discriminates between cues.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_decision_models.png"))

```

```{r scenarios, fig.cap = "Decision scenarios according to the best discriminating feature between cues.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "Z_scenarios.png"))

```

<!-- Results -->
<!-- Model Classification -->
```{r classification, fig.cap = "Strategy group classification based on the variational Bayesian inference.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "A1_model_histogram.png"))

```

```{r classification-reduced, fig.cap = "Decision groups studied in detail for this work.", echo=FALSE, warning=F, message=F}
knitr::include_graphics(file.path(folder_figures, "A2_model_histogram.png"))
```


<!-- Performance -->
```{r performance, fig.cap = "Performance (as percent of accurate trials) across testing phases. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "B1_performance.png"))
```

```{r table-performance-change, results='asis', echo=F}
apa_table(rope_performance_change, caption = "Accuracy between phases across groups", longtable = T)
```

```{r table-performance-group-diff, results='asis', echo=F}
apa_table(rope_performance_group_diff, caption = "Paired comparisons between groups on the difference in performance across phases", longtable = T)
```

```{r table-performance-T1, results='asis', echo=F}
apa_table(rope_performance_T1, caption = "Paired comparisons between groups on the performance in T1", longtable = T)
```


<!-- First fixation -->
```{r first, fig.cap = "Allocation of first fixations across features in T1. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "C1_first.png"))
```

```{r first-scenario, fig.cap = "Allocation of first fixations across features in the different decision scenarios in T1. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "C2_first_scenario.png"))
```

```{r table-first, results='asis', echo=F}
apa_table(rope_first, caption = "Follow up comparisons for the proportion of first fixations across decision scenarios.", longtable = T)
```

```{r table-first-scenario, results='asis', echo=F}
apa_table(rope_first_scenario, caption = "Follow up comparisons for the proportion of first fixations.", longtable = T) 
```


<!-- Proportion -->
```{r proportion, fig.cap = "Allocation of fixations across features in T1 for the different decision groups. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "D1_proportion.png"))
```

```{r proportion-scenario, fig.cap = "Allocation of fixations across features in T1 for the different decision groups. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "D2_proportion_scenario.png"))
```

```{r table-proportion, results='asis', echo=F}
apa_table(rope_proportion, caption = "Follow up comparisons for the proportion of fixations.", longtable = T) 
```

```{r table-proportion-scenario, results='asis', echo=F}
apa_table(rope_proportion_scenario, caption = "Follow up comparisons for the proportion of fixations across decision scenarios.", longtable = T) 
```


<!-- Last Fixation -->
```{r last, fig.cap = "Allocation of the last fixations across features in T1. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "E1_last.png"))
```

```{r last-scenario, fig.cap = "Allocation of last fixations across features in the different decision scenarios in T1. Individual dots correspond to the median from the posterior draws from each participant. Error bars represent standard deviations.", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics(file.path(folder_figures, "E2_last_scenario.png"))
```

```{r table-last, results='asis', echo=F}
apa_table(rope_last, caption = "Follow up comparisons for the last fixations.", longtable = T) 
```

```{r table-last-scenario, results='asis', echo=F}
apa_table(rope_last_scenario, caption = "Follow up comparisons for the proportion of last fixations across decision scenarios.", longtable = T) 
```

